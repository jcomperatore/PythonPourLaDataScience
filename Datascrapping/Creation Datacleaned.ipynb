{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création fichier csv clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook est consacré à la création du fichier qui sera utilisé comme base de données. Pour cela, nou savons pris le format CSV, avec comme séparateur la barre verticale \"|\" \n",
    "Il est séparé en plusieurs étapes : \n",
    "\n",
    "- Installations et importations\n",
    "- Fonctions utilitaires\n",
    "- Script permettant l'écriture du fichier\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations et importations {#petit1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations pour la récupération des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installations pour la récupération des données\n",
    "\n",
    "%pip install webdriver-manager\n",
    "%pip install -q lxml\n",
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations de spacy et du package des mots français inutiles pour le cleaning des questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation pour le cleaning\n",
    "\n",
    "%pip install spacy \n",
    "%python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des modules nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import lxml\n",
    "import pandas\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs fonctions utilitaires sont définies ici : celle qui permet d'obtenir le lien d'une page, pour plus de lisibilité, et celle qui permet de nettoyer le texte contenu dans une question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonctions utilitaires\n",
    "\n",
    "def pageurl(x) : \n",
    "    return \"https://questions.assemblee-nationale.fr/q16/16-\"+str(x)+\"QE.htm\"\n",
    "\n",
    "\n",
    "def clean_question(text, nom, remove_stopwords=True):\n",
    "    \n",
    "    text = nlp(text)\n",
    "\n",
    "    stopwords = nlp.Defaults.stop_words \n",
    "    stopwords = stopwords.union(set(nom.lower().split()+nom.split()+['m.','mme','ministre','été', 'cas',\"redemande\",\"interroge\", \"demande\", \"luire\", \"année\", \"faire\", \"nombre\",\"ailleurs\",\"dernier\",\"concerner\",\"situation\",\"dispositif\",\"aujourd'hui\",\"part\",\"pouvoir\",\"permettre\",'ans',\"jour\",\"appeller\", \"attention\", \"attirer\", \"non\", \"pourtant\"]))\n",
    "    \n",
    "    tokens = [tok.lemma_ for tok in text if not tok.is_punct]\n",
    "    if remove_stopwords:\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "    question_clean = ' '.join(tokens)\n",
    "    return question_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du fichier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée d'abord le fichier, en précisant l'encodage pour éviter les problemes d'accents. Ensuite, on applique le script de scraping que l'on a pu déjà réalise dans le notebook [`Scraping Dataframe`](https://github.com/jcomperatore/PythonPourLaDataScience/blob/main/Datascrapping/Scraping%20Dataframe.ipynb), auquel on rajoute l'étape de nettoyage, a la ligne 34.\n",
    "\n",
    "Pour éviter de modifier le fichier final, tous les \"data_cleaned.csv\" seront remplacés par des \"data_cleaned_TEST.csv\" pour le lecteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création fichier\n",
    "\n",
    "données = open(\"data_cleaned_TEST.csv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "données.write(\"id|groupe|nom|ministère_interrogé|rubrique|titre|date|question|question_clean\")\n",
    "\n",
    "données.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour tester ce code (assez long),  il est recomandé d'essayer pour un petit nombre de questions (une cinquantaine prend une minute, raisonnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "données = open(\"data_cleaned_TEST.csv\", \"a\", encoding=\"utf-8\")\n",
    "\n",
    "for k in range(1, 51) : \n",
    "    \n",
    "    source_code = request.urlopen(pageurl(k)).read()\n",
    "    page = bs4.BeautifulSoup(source_code)\n",
    "    question0 = page.find(\"div\", {\"class\" : \"question\"})\n",
    "    question = question0.find(\"p\")\n",
    "    question = question.text.strip()\n",
    "    \n",
    "    rubrique0 = page.findAll(\"div\", {\"class\" : \"question_col33\"})\n",
    "    rubrique,titre = rubrique0[0].find(\"p\"), rubrique0[1].find(\"p\")\n",
    "    rubrique, titre = rubrique.text.strip()[11:], titre.text.strip()[8:]\n",
    "    \n",
    "    ministere0 = page.find(\"div\", {\"class\" : \"question_col50\"})\n",
    "    ministere = ministere0.text.strip()[23:]\n",
    "\n",
    "    lien0 = page.find(\"span\", {\"class\" : \"question_big_content\"})\n",
    "    lien = lien0.find(\"a\").get(\"href\")\n",
    "\n",
    "    date0 = page.find(\"div\", {\"class\" : \"question_publish_date\"})\n",
    "    date = date0.find(\"span\")\n",
    "    date = date.text.strip()\n",
    "    \n",
    "    source_code_parlementaire = request.urlopen(lien).read()\n",
    "    page_parlementaire = bs4.BeautifulSoup(source_code_parlementaire)\n",
    "\n",
    "    groupe0 = page_parlementaire.find(\"a\", {\"class\" : \"h4 _colored link\"})\n",
    "    groupe = groupe0.text.strip()\n",
    "\n",
    "    nom0 = page_parlementaire.find(\"h1\", {\"class\" : \"h1 _mt-small\"})\n",
    "    nom = nom0.text.strip()\n",
    "\n",
    "    question_clean = clean_question(question, nom)\n",
    "\n",
    "    données.write(\"\\n\" + str(k) + \"|\" + groupe + \"|\" + nom +  \"|\" + ministere + \"|\" + rubrique + \"|\" + titre + \"|\" + date + \"|\" + question + \"|\" + question_clean)\n",
    "\n",
    "données.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
