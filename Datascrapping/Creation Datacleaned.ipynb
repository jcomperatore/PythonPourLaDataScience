{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création fichier csv clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook est consacré à la création du fichier qui sera utilisé comme base de données. Pour cela, nou savons pris le format CSV, avec comme séparateur la barre verticale \"|\" \n",
    "Il est séparé en plusieurs étapes : \n",
    "\n",
    "    1) [Installations et importations](#petit)\n",
    "    2) Fonctions utilitaires\n",
    "    3) Script permettant l'écriture du fichier\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Installations et importations {#petit1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besoin d'installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installations pour la récupération des données\n",
    "\n",
    "!pip install webdriver-manager\n",
    "!pip install -q lxml\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation pour le cleaning\n",
    "\n",
    "!pip install spacy \n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import lxml\n",
    "import pandas\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonctions utilitaires\n",
    "\n",
    "def pageurl(x) : \n",
    "    return \"https://questions.assemblee-nationale.fr/q16/16-\"+str(x)+\"QE.htm\"\n",
    "\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "def clean_docs(texts, ministere, remove_stopwords=True, n_process = 4):\n",
    "    \n",
    "    docs = nlp.pipe(texts, \n",
    "                    n_process=n_process,\n",
    "                    disable=['parser', 'ner', 'textcat'])\n",
    "    stopwords = nlp.Defaults.stop_words.union(set(['m.','mme','ministre', ministere.lower()]))\n",
    "\n",
    "    docs_cleaned = []\n",
    "    for doc in docs:\n",
    "        tokens = [tok.text.lower().strip() for tok in doc if not tok.is_punct]\n",
    "        if remove_stopwords:\n",
    "            tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "        doc_clean = ' '.join(tokens)\n",
    "        docs_cleaned.append(doc_clean)\n",
    "        \n",
    "    return docs_cleaned\n",
    "\n",
    "# pas sur de l'utilité : mots qui peuvent etre quand meme utile (souveraineté ?), appelle parfois le délégué des finances alors qu'interroge le ministere des comptes publics\n",
    "\"\"\"def clean_question2(text, ministere, nom, remove_stopwords=True, n_process=4):\n",
    "    \n",
    "    text = nlp(text)\n",
    "\n",
    "    inclus = ministere.lower().split() + nom.lower().split()\n",
    "    print(inclus)\n",
    "    stopwords = nlp.Defaults.stop_words \n",
    "    stopwords = stopwords.union(set(['m.','mme','ministre']+inclus))\n",
    "    \n",
    "    tokens = [tok.text.lower().strip() for tok in text if not tok.is_punct]\n",
    "    if remove_stopwords:\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "    question_clean = ' '.join(tokens)\n",
    "    return question_clean\"\"\"\n",
    "\n",
    "def clean_question(text, nom, remove_stopwords=True, n_process=4):\n",
    "    \n",
    "    text = nlp(text)\n",
    "\n",
    "    stopwords = nlp.Defaults.stop_words \n",
    "    stopwords = stopwords.union(set(['m.','mme','ministre']+nom.lower().split()))\n",
    "    \n",
    "    tokens = [tok.text.lower().strip() for tok in text if not tok.is_punct]\n",
    "    if remove_stopwords:\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords]\n",
    "    question_clean = ' '.join(tokens)\n",
    "    return question_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création fichier\n",
    "\n",
    "données = open(\"data_cleaned.csv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "données.write(\"id|groupe|nom|ministère_interrogé|rubrique|titre|date|question|question_clean\")\n",
    "\n",
    "données.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "données = open(\"data_cleaned.csv\", \"a\", encoding=\"utf-8\")\n",
    "\n",
    "for k in range(500, 1500) : \n",
    "    source_code = request.urlopen(pageurl(k)).read()\n",
    "    page = bs4.BeautifulSoup(source_code)\n",
    "    question0 = page.find(\"div\", {\"class\" : \"question\"})\n",
    "    question = question0.find(\"p\")\n",
    "    question = question.text.strip()\n",
    "    \n",
    "    rubrique0 = page.findAll(\"div\", {\"class\" : \"question_col33\"})\n",
    "    rubrique,titre = rubrique0[0].find(\"p\"), rubrique0[1].find(\"p\")\n",
    "    rubrique, titre = rubrique.text.strip()[11:], titre.text.strip()[8:]\n",
    "    \n",
    "    ministere0 = page.find(\"div\", {\"class\" : \"question_col50\"})\n",
    "    ministere = ministere0.text.strip()[23:]\n",
    "\n",
    "    lien0 = page.find(\"span\", {\"class\" : \"question_big_content\"})\n",
    "    lien = lien0.find(\"a\").get(\"href\")\n",
    "\n",
    "    date0 = page.find(\"div\", {\"class\" : \"question_publish_date\"})\n",
    "    date = date0.find(\"span\")\n",
    "    date = date.text.strip()\n",
    "    \n",
    "    source_code_parlementaire = request.urlopen(lien).read()\n",
    "    page_parlementaire = bs4.BeautifulSoup(source_code_parlementaire)\n",
    "\n",
    "    groupe0 = page_parlementaire.find(\"a\", {\"class\" : \"h4 _colored link\"})\n",
    "    groupe = groupe0.text.strip()\n",
    "\n",
    "    nom0 = page_parlementaire.find(\"h1\", {\"class\" : \"h1 _mt-small\"})\n",
    "    nom = nom0.text.strip()\n",
    "\n",
    "    question_clean = clean_question(question, nom)\n",
    "\n",
    "\n",
    "    données.write(\"\\n\" + str(k) + \"|\" + groupe + \"|\" + nom +  \"|\" + ministere + \"|\" + rubrique + \"|\" + titre + \"|\" + date + \"|\" + question + \"|\" + question_clean)\n",
    "\n",
    "données.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
